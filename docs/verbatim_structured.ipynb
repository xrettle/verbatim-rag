{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verbatim RAG: Structured Templates & VerbatimDoc\n",
    "\n",
    "This notebook demonstrates the two new features in v0.1.8:\n",
    "\n",
    "1. **Structured Templates** - Control exactly how your answers are formatted with semantic placeholders like `[CONTRIBUTIONS]`, `[METHODOLOGY]`, `[RESULTS]`\n",
    "\n",
    "2. **VerbatimDoc** - Generate complete documents from templates with embedded queries like `[!query=what methodology was used]`\n",
    "\n",
    "Every fact in the output is verbatim from your documents. Traceable, no hallucinations.\n",
    "\n",
    "For a comprehensive tutorial, see [build_verbatim.ipynb](./build_verbatim.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"verbatim-rag>=0.1.8\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "# Suppress verbose logs\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"milvus\").setLevel(logging.WARNING)\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-key-here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamkovacs/miniconda3/envs/verb/lib/python3.11/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n",
      "2025-12-05 22:26:40,750 - INFO - Created indexes for collection: verbatim_rag\n",
      "2025-12-05 22:26:40,753 - INFO - Created documents collection: verbatim_rag_documents\n",
      "2025-12-05 22:26:40,754 - INFO - Connected to Milvus Lite: ./v018_demo.db\n",
      "2025-12-05 22:26:40,861 - INFO - PyTorch version 2.8.0 available.\n",
      "2025-12-05 22:26:41,349 - INFO - Load pretrained SparseEncoder: opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill\n",
      "2025-12-05 22:26:44,644 - INFO - Loaded SPLADE model: opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill\n",
      "2025-12-05 22:26:45,640 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-05 22:26:45,689 - INFO - Going to convert document batch...\n",
      "2025-12-05 22:26:45,690 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e647edf348883bed75367b22fbe60347\n",
      "2025-12-05 22:26:45,698 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-05 22:26:45,700 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-12-05 22:26:45,707 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-05 22:26:45,710 - INFO - Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-12-05 22:26:46,167 - INFO - Accelerator device: 'mps'\n",
      "2025-12-05 22:26:48,812 - INFO - Accelerator device: 'mps'\n",
      "2025-12-05 22:26:50,283 - INFO - Accelerator device: 'mps'\n",
      "2025-12-05 22:26:50,907 - INFO - Processing document 2025.bionlp-share.8.pdf\n",
      "2025-12-05 22:26:59,668 - INFO - Finished converting document 2025.bionlp-share.8.pdf in 15.01 sec.\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.11s/it]]\n",
      "2025-12-05 22:27:03,846 - INFO - Added 19 vectors to Milvus\n",
      "2025-12-05 22:27:03,848 - INFO - Added 1 documents to Milvus\n",
      "Adding documents: 100%|██████████| 1/1 [00:04<00:00,  4.15s/it]\n"
     ]
    }
   ],
   "source": [
    "from verbatim_rag import VerbatimRAG, VerbatimIndex\n",
    "from verbatim_rag.ingestion import DocumentProcessor\n",
    "from verbatim_rag.vector_stores import LocalMilvusStore\n",
    "from verbatim_rag.embedding_providers import SpladeProvider\n",
    "from verbatim_rag.llm_client import LLMClient\n",
    "\n",
    "store = LocalMilvusStore(db_path=\"./v018_demo.db\", enable_sparse=True, enable_dense=False)\n",
    "\n",
    "llm_client = LLMClient(\n",
    "    model=\"gpt-5.1\",\n",
    "    temperature=1.0,\n",
    ")\n",
    "\n",
    "sparse_provider = SpladeProvider(\n",
    "    model_name=\"opensearch-project/opensearch-neural-sparse-encoding-doc-v2-distill\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "index = VerbatimIndex(vector_store=store, sparse_provider=sparse_provider)\n",
    "\n",
    "# Add a sample research paper\n",
    "doc = DocumentProcessor().process_url(\n",
    "    \"https://aclanthology.org/2025.bionlp-share.8.pdf\",\n",
    "    title=\"KR Labs at ArchEHR-QA 2025\"\n",
    ")\n",
    "index.add_documents([doc])\n",
    "\n",
    "# Create RAG instance\n",
    "rag = VerbatimRAG(index, llm_client=llm_client)\n",
    "rag.template_manager.citation_mode = \"hidden\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feature 1: Structured Templates\n",
    "\n",
    "Control exactly how your answers are formatted. Define sections with semantic placeholders and get a structured response with each section filled from your sources.\n",
    "\n",
    "**How it works:**\n",
    "- You provide a template with placeholders like `[CONTRIBUTIONS]`, `[METHODOLOGY]`, `[RESULTS]`\n",
    "- You ask a single question\n",
    "- The LLM extracts relevant spans for each section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.99it/s]\n",
      "2025-12-05 22:29:13,823 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Metric | Baseline | Ours |\n",
      "|--------|----------|------|\n",
      "| F1 | 33.6 | 52.1 |\n"
     ]
    }
   ],
   "source": [
    "# Define your structured template\n",
    "rag.template_manager.use_structured_mode(template=\"\"\"\n",
    "| Metric | Baseline | Ours |\n",
    "|--------|----------|------|\n",
    "| F1 | [BASELINE_F1] | [OUR_F1] |\n",
    "\"\"\")\n",
    "\n",
    "# Single query - template guides what to extract\n",
    "response = await rag.query_async(\"What is the F1 score of the baseline and our model?\")\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feature 2: VerbatimDoc\n",
    "\n",
    "Generate complete documents from templates with embedded queries. Each `[!query=...]` becomes a separate RAG query, and the results are composed into a final document.\n",
    "\n",
    "**How it works:**\n",
    "- You write a document template with embedded queries\n",
    "- Each query is processed independently\n",
    "- Results are inserted with global citation numbering\n",
    "- Useful for research summaries, reports, literature reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 10.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting spans (async batch mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting spans (async batch mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting spans (async batch mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-05 22:30:02,457 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-05 22:30:02,708 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-12-05 22:30:03,708 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Quick Summary\n",
      "\n",
      "## What's New?\n",
      "[1] Our contributions include a modular, traceable QA architecture that mitigates hallucinations, a method to generate synthetic EHR question-answer corpus and train custom models. Additionally, we are releasing all the code on GitHub 2 under the MIT License.\n",
      "\n",
      "## How?\n",
      "[2] To tackle this problem, we propose a verbatim pipeline that clearly separates extraction and generation to mitigate hallucinations:\n",
      "\n",
      "- Sentence-level extraction , using either zeroshot LLMs or supervised ModernBERT classifiers.\n",
      "- Template-constrained generation , dynamically creating answer templates filled exclu-\n",
      "\n",
      "sively with verbatim sentences selected from the extraction phase.\n",
      "\n",
      "We participated in the ArchEHR-QA 2025 shared task on grounded question answering (QA) from electronic health records (EHRs). Our approach involved (i) utilizing a zero-shot gemma-3-27b-it 1 LLM (Team et al., 2025) and (ii) generating synthetic data for sentence extraction from EHRs to train a compact extractor. For this purpose, we trained a Clinical ModernBERT classifier (Lee et al., 2025; Warner et al., 2024), achieving performance comparable to the LLM extractor. Both extractors were then fed into the same LLM template generator.\n",
      "\n",
      "## Results?\n",
      "[3] Interestingly, final test scores were closely matched between our extractors: the LLM-based model scored 42.01%, while Clinical ModernBERT achieved a near-identical 41.85%.\n",
      "\n",
      "[4] Our best system (zero-shot LLM based on gemma-3-27b-it ) scored 42.01% overall, placing within the top 10 in multiple core metrics and significantly outperforming the organizers' baseline (a 70B-parameter Llama-3.3 model) across most metrics.\n",
      "\n",
      "[5] Our final submission employed the LLM extractor for its balanced performance.\n",
      "\n",
      "[6] Our solution achieved an overall score of 42.01% , ranking in the top 10 for core metrics, and surpassed the organizers' 70Bparameter Llama-3.3 baseline by a large margin.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from verbatim_rag.verbatim_doc import VerbatimDOC\n",
    "\n",
    "# Define document template with embedded queries\n",
    "template = \"\"\"\n",
    "# Quick Summary\n",
    "\n",
    "## What's New?\n",
    "[!query=what is the main contribution]\n",
    "\n",
    "## How?\n",
    "[!query=what methodology was used]\n",
    "\n",
    "## Results?\n",
    "[!query=what accuracy was achieved]\n",
    "\"\"\"\n",
    "\n",
    "# Generate the document\n",
    "result = await VerbatimDOC(rag).process(template, auto_approve=True)\n",
    "print(result.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Structured Templates vs VerbatimDoc\n",
    "\n",
    "| | Structured Templates | VerbatimDoc |\n",
    "|---|---|---|\n",
    "| **Queries** | 1 query, template guides extraction | N independent queries |\n",
    "| **Template** | Semantic placeholders: `[METHODOLOGY]` | Embedded queries: `[!query=...]` |\n",
    "| **Use case** | Structured extraction from same context | Multi-section documents with different questions |\n",
    "| **Best for** | Summaries, analysis | Reports, literature reviews |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learn More\n",
    "\n",
    "- **Full Tutorial:** [build_verbatim.ipynb](./build_verbatim.ipynb)\n",
    "- **GitHub:** [github.com/KRLabsOrg/verbatim-rag](https://github.com/KRLabsOrg/verbatim-rag)\n",
    "- **Blog Post:** [huggingface.co/blog/adaamko/verbatimrag](https://huggingface.co/blog/adaamko/verbatimrag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
