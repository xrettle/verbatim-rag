{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Verbatim: A Developer's Guide to Hallucination-Free RAG\n",
    "\n",
    "*A hands-on tutorial for developers who want to build trustworthy, verifiable RAG systems*\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/KRLabsOrg/verbatim-rag/blob/main/assets/chiliground.png?raw=true\" alt=\"ChiliGround Logo\" width=\"400\"/>\n",
    "  <br><em>Chill, I Ground!</em>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Installation\n",
    "\n",
    "First, install Verbatim RAG and its dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"verbatim-rag==0.1.6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress verbose logs for cleaner output\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"milvus\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup\n",
    "\n",
    "If you plan to use LLM-based extraction (recommended for best accuracy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "# Or load from .env file\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: The Hallucination Problem\n",
    "\n",
    "If you've built RAG systems before, you've probably seen this:\n",
    "\n",
    "**Question:** \"How much synthetic data was generated?\"\n",
    "\n",
    "**Traditional RAG Answer:** \"They generated around 58,000 synthetic training examples...\"\n",
    "\n",
    "**The actual source:** \"We constructed a comprehensive dataset of 60k synthetic training examples...\"\n",
    "\n",
    "Notice the problem? The LLM rounded \"60k\" to \"around 58,000\" (wrong direction!). In medical, legal, or financial domains, this approximation could be catastrophic.\n",
    "\n",
    "### Even with Perfect Retrieval, LLMs Still Hallucinate\n",
    "\n",
    "Even with retrieval-augmented generation, LLMs can still:\n",
    "- Round or approximate numbers\n",
    "- Combine information incorrectly\n",
    "- Fill in gaps with training knowledge\n",
    "- Generate plausible-sounding but false information\n",
    "\n",
    "**What if we could force the LLM to only use exact text from the source documents?**\n",
    "\n",
    "That's exactly what Verbatim RAG does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side-by-Side Comparison\n",
    "\n",
    "Let's see the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional RAG (typical output)\n",
    "traditional_answer = \"\"\"\n",
    "The study generated around 58,000-60,000 synthetic examples \n",
    "using various data augmentation techniques.\n",
    "\"\"\"\n",
    "\n",
    "# Verbatim RAG (actual output)\n",
    "verbatim_answer = \"\"\"\n",
    "[1] We constructed a comprehensive dataset of 60k synthetic \n",
    "training examples.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Traditional RAG:\")\n",
    "print(traditional_answer)\n",
    "print(\"Problems: Rounded numbers, paraphrased terms, no exact source\")\n",
    "print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "print(\"Verbatim RAG:\")\n",
    "print(verbatim_answer)\n",
    "print(\"Result: Exact quote from source, verifiable, with citation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Verbatim Solution\n",
    "\n",
    "Instead of letting the LLM freely generate text based on retrieved documents, Verbatim RAG:\n",
    "\n",
    "1. **Extracts exact text spans** from source documents\n",
    "2. **Composes answers** entirely from these verbatim passages\n",
    "3. **Provides precise citations** linking back to sources\n",
    "\n",
    "The result? Zero hallucinations. Every claim is directly traceable to the source text.\n",
    "\n",
    "**Key difference:**\n",
    "- Traditional RAG: Retrieve → Generate (LLM can invent)\n",
    "- Verbatim RAG: Retrieve → Extract → Compose (LLM only quotes)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Complete System (5-Minute Quickstart)\n",
    "\n",
    "Let's see Verbatim RAG in action. We'll build a complete hallucination-free system in under 15 lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamkovacs/miniconda3/envs/verb/lib/python3.11/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n",
      "2025-11-17 12:17:03,612 - INFO - Created indexes for collection: verbatim_rag\n",
      "2025-11-17 12:17:03,620 - INFO - Created documents collection: verbatim_rag_documents\n",
      "2025-11-17 12:17:03,620 - INFO - Connected to Milvus Lite: ./demo.db\n",
      "2025-11-17 12:17:03,678 - INFO - PyTorch version 2.8.0 available.\n",
      "2025-11-17 12:17:04,431 - INFO - Load pretrained SparseEncoder: naver/splade-v3\n",
      "2025-11-17 12:17:06,889 - INFO - Loaded SPLADE model: naver/splade-v3\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.40it/s]]\n",
      "2025-11-17 12:17:08,104 - INFO - Added 3 vectors to Milvus\n",
      "2025-11-17 12:17:08,107 - INFO - Added 1 documents to Milvus\n",
      "Adding documents: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting relevant spans...\n",
      "Extracting spans (batch mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 12:17:10,818 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spans...\n",
      "Generating response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 12:17:12,197 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The approaches used in this context include the following:\n",
      "\n",
      "1. **Fact 1:** [1] We used two approaches: zero-shot LLM extraction and a fine-tuned ModernBERT classifier on 58k synthetic examples.\n"
     ]
    }
   ],
   "source": [
    "from verbatim_rag import VerbatimRAG, VerbatimIndex\n",
    "from verbatim_rag.vector_stores import LocalMilvusStore\n",
    "from verbatim_rag.embedding_providers import SpladeProvider\n",
    "from verbatim_rag.schema import DocumentSchema\n",
    "\n",
    "# 1. Create index (storage layer)\n",
    "store = LocalMilvusStore(\"./demo.db\", enable_sparse=True, enable_dense=False)\n",
    "embedder = SpladeProvider(\"naver/splade-v3\", device=\"cpu\")\n",
    "index = VerbatimIndex(vector_store=store, sparse_provider=embedder)\n",
    "\n",
    "# 2. Add a document\n",
    "doc = DocumentSchema(\n",
    "    content=\"\"\"\n",
    "# Methods\n",
    "We used two approaches: zero-shot LLM extraction and a fine-tuned ModernBERT classifier on 58k synthetic examples.\n",
    "    \n",
    "# Results\n",
    "The system achieved 42.01% accuracy on ArchEHR-QA.\n",
    "\"\"\",\n",
    "    title=\"Research Paper\",\n",
    ")\n",
    "index.add_documents([doc])\n",
    "\n",
    "# 3. Create RAG (add intelligence layer)\n",
    "rag = VerbatimRAG(index, model=\"gpt-4o-mini\")\n",
    "\n",
    "# 4. Ask questions and get exact quotes\n",
    "response = rag.query(\"What approaches were used?\")\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Its very easy to use dense embeddings instead\n",
    "from verbatim_rag.embedding_providers import SentenceTransformersProvider\n",
    "\n",
    "embedder_dense = SentenceTransformersProvider(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# The embedding dimension needs to be defined\n",
    "store_dense = LocalMilvusStore(\n",
    "    \"./demo_dense.db\",\n",
    "    enable_sparse=False,\n",
    "    enable_dense=True,\n",
    "    dense_dim=embedder_dense.get_dimension(),\n",
    ")\n",
    "index_dense = VerbatimIndex(vector_store=store_dense, dense_provider=embedder_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Want to use dense embeddings instead?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Architecture Deep Dive - The Two-Layer System\n",
    "\n",
    "Verbatim RAG is built on **two independent layers**. Understanding this separation is key to using the system effectively.\n",
    "\n",
    "### Visual Overview\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/KRLabsOrg/verbatim-rag/blob/main/assets/verbatim_architecture.png?raw=true\" alt=\"Verbatim RAG Architecture\" width=\"800\"/>\n",
    "</p>\n",
    "\n",
    "The architecture shows three main integration patterns:\n",
    "\n",
    "1. **Full System**: User → VerbatimIndex + Verbatim Core → Answer\n",
    "2. **Custom RAG Provider**: User → Your existing RAG → Verbatim Core → Answer\n",
    "3. **VerbatimIndex Only**: User → VerbatimIndex (Chonkie + Milvus + Docling) → Retrieved chunks\n",
    "\n",
    "### Layer 1: Storage (VerbatimIndex)\n",
    "\n",
    "**Job:** Find relevant documents\n",
    "\n",
    "| Component | What It Does | Options |\n",
    "|-----------|--------------|----------|\n",
    "| Chunker | Splits documents intelligently | Markdown, Simple, Chonkie |\n",
    "| Embedder | Converts text to searchable vectors | SPLADE (sparse), SentenceTransformers (dense), OpenAI |\n",
    "| Vector Store | Stores and retrieves efficiently | LocalMilvus, CloudMilvus |\n",
    "\n",
    "**Input:** Documents  \n",
    "**Output:** Relevant chunks for a query\n",
    "\n",
    "### Layer 2: Verbatim Core (VerbatimRAG)\n",
    "\n",
    "**Job:** Extract verbatim answers\n",
    "\n",
    "| Component | What It Does | Options |\n",
    "|-----------|--------------|----------|\n",
    "| Span Extractor | Identifies exact text that answers questions | LLM-based, ModernBERT-based |\n",
    "| Template Manager | Structures extracted spans | Static, Dynamic, Question-specific |\n",
    "| Verification | Ensures spans exist verbatim in source | Automatic |\n",
    "\n",
    "**Input:** Question + Relevant chunks  \n",
    "**Output:** Verbatim answer with citations\n",
    "\n",
    "### Why This Separation Matters\n",
    "\n",
    "**1. Modularity:** Swap any component without affecting others\n",
    "- Want GPU embeddings? Just change the embedder\n",
    "- Want cloud storage? Just change the vector store\n",
    "- Want custom extraction? Just change the span extractor\n",
    "\n",
    "**2. Flexibility:** Use just the parts you need\n",
    "- Already have retrieval? Use only Verbatim Core\n",
    "- Need better chunking? Use only VerbatimIndex\n",
    "- Want full system? Use both layers\n",
    "\n",
    "**3. Integration:** Wrap your existing RAG\n",
    "- Keep your LangChain/LlamaIndex retrieval\n",
    "- Add Verbatim Core on top\n",
    "- Get hallucination-free answers without rebuilding\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Verbatim Core - How Verbatim Prevents Hallucinations\n",
    "\n",
    "This is the core innovation. Let's see exactly how span extraction prevents hallucinations.\n",
    "\n",
    "### The Problem with Free Generation\n",
    "\n",
    "Traditional RAG lets the LLM generate freely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulation of what traditional RAG does\n",
    "retrieved_context = \"We created 58k examples. The system used synthetic data.\"\n",
    "\n",
    "# LLM generates:\n",
    "traditional_answer = \"They generated approximately 58,000 synthetic training samples\"\n",
    "#                     ↑           ↑            ↑           ↑\n",
    "#                 Invented   Approximated   Changed    Added word\n",
    "\n",
    "print(\"Retrieved:\", retrieved_context)\n",
    "print(\"Generated:\", traditional_answer)\n",
    "print(\"\\nProblems:\")\n",
    "print(\"- Changed 'We' to 'They'\")\n",
    "print(\"- Changed '58k' to 'approximately 58,000'\")\n",
    "print(\"- Changed 'created' to 'generated'\")\n",
    "print(\"- Added 'training' (not in source)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Verbatim Solution: Extract, Don't Generate\n",
    "\n",
    "Verbatim RAG constrains the LLM to extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same context\n",
    "retrieved_context = \"We created 58k examples. The system used synthetic data.\"\n",
    "\n",
    "# LLM extracts (doesn't generate):\n",
    "extracted_spans = [\"We created 58k examples\"]\n",
    "\n",
    "# Answer composed from exact quotes:\n",
    "verbatim_answer = \"[1] We created 58k examples.\"\n",
    "#                  ↑    Every word is exact    ↑\n",
    "\n",
    "print(\"Retrieved:\", retrieved_context)\n",
    "print(\"Extracted:\", extracted_spans)\n",
    "print(\"Answer:\", verbatim_answer)\n",
    "print(\"\\nResult: Every word is verbatim from the source.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step: How Extraction Works\n",
    "\n",
    "Let's see the extraction process in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting spans (batch mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 11:49:14,052 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What methods were used?\n",
      "\n",
      "Extracted spans:\n",
      "  - We used a zero-shot LLM approach for span extraction.\n",
      "  - We fine-tuned ModernBERT on 58k synthetic examples.\n",
      "\n",
      "Note: 'The system runs on CPU' was rejected - doesn't answer the question\n"
     ]
    }
   ],
   "source": [
    "from verbatim_rag.extractors import LLMSpanExtractor\n",
    "from verbatim_rag.vector_stores import SearchResult\n",
    "\n",
    "# Question and retrieved chunks\n",
    "question = \"What methods were used?\"\n",
    "\n",
    "# Simulate retrieved chunks\n",
    "chunks = [\n",
    "    SearchResult(\n",
    "        id=\"1\",\n",
    "        score=0.95,\n",
    "        text=\"We used a zero-shot LLM approach for span extraction.\",\n",
    "        metadata={\"title\": \"Methods\"},\n",
    "    ),\n",
    "    SearchResult(\n",
    "        id=\"2\",\n",
    "        score=0.90,\n",
    "        text=\"We fine-tuned ModernBERT on 58k synthetic examples.\",\n",
    "        metadata={\"title\": \"Methods\"},\n",
    "    ),\n",
    "    SearchResult(\n",
    "        id=\"3\",\n",
    "        score=0.60,\n",
    "        text=\"The system runs on CPU.\",\n",
    "        metadata={\"title\": \"Implementation\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Extract spans\n",
    "extractor = LLMSpanExtractor(model=\"gpt-4o-mini\")\n",
    "spans = extractor.extract_spans(question, chunks)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"\\nExtracted spans:\")\n",
    "for doc_text, span_list in spans.items():\n",
    "    for span in span_list:\n",
    "        print(f\"  - {span}\")\n",
    "\n",
    "print(\"\\nNote: 'The system runs on CPU' was rejected - doesn't answer the question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Template Composition\n",
    "\n",
    "Once spans are extracted, the template arranges them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted spans:\n",
      "  chunk_1: We used a zero-shot LLM approach for span extraction\n",
      "  chunk_2: We fine-tuned ModernBERT on 58k synthetic examples\n",
      "\n",
      "Composed answer:\n",
      "\n",
      "The methods used include:\n",
      "\n",
      "[1] We used a zero-shot LLM approach for span extraction.\n",
      "[2] We fine-tuned ModernBERT on 58k synthetic examples.\n",
      "\n",
      "Note: Every word is verbatim from source. No generation occurred.\n"
     ]
    }
   ],
   "source": [
    "# Extracted spans (from above)\n",
    "spans_dict = {\n",
    "    \"chunk_1\": [\"We used a zero-shot LLM approach for span extraction\"],\n",
    "    \"chunk_2\": [\"We fine-tuned ModernBERT on 58k synthetic examples\"],\n",
    "}\n",
    "\n",
    "# Template composes them\n",
    "final_answer = \"\"\"\n",
    "The methods used include:\n",
    "\n",
    "[1] We used a zero-shot LLM approach for span extraction.\n",
    "[2] We fine-tuned ModernBERT on 58k synthetic examples.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Extracted spans:\")\n",
    "for chunk_id, span_list in spans_dict.items():\n",
    "    print(f\"  {chunk_id}: {span_list[0]}\")\n",
    "\n",
    "print(\"\\nComposed answer:\")\n",
    "print(final_answer)\n",
    "print(\"Note: Every word is verbatim from source. No generation occurred.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight: The LLM Never Generates Facts\n",
    "\n",
    "**Traditional RAG:**\n",
    "- LLM understands the documents\n",
    "- LLM generates an answer (risk of hallucination)\n",
    "\n",
    "**Verbatim RAG:**\n",
    "- LLM understands the documents\n",
    "- LLM identifies relevant text spans (no risk - just selection)\n",
    "- System composes answer from verbatim quotes\n",
    "\n",
    "The generation step is replaced by verbatim composition.\n",
    "\n",
    "### Choosing Your Extractor\n",
    "\n",
    "You have two options for span extraction:\n",
    "\n",
    "**Option 1: LLM-based Extractor** (default)\n",
    "\n",
    "```python\n",
    "rag = VerbatimRAG(index, model=\"gpt-4o-mini\")\n",
    "```\n",
    "\n",
    "Pros:\n",
    "- Zero-shot: works immediately without training\n",
    "- Handles complex reasoning\n",
    "- Best accuracy\n",
    "\n",
    "Cons:\n",
    "- Requires API calls (costs money)\n",
    "- Slower\n",
    "\n",
    "**Option 2: ModernBERT Extractor** (CPU-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from KRLabsOrg/verbatim-rag-modern-bert-v1...\n",
      "Loading tokenizer from KRLabsOrg/verbatim-rag-modern-bert-v1...\n",
      "Tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from verbatim_rag.extractors import ModelSpanExtractor\n",
    "\n",
    "# Use our fine-tuned ModernBERT model\n",
    "extractor = ModelSpanExtractor(\n",
    "    model_path=\"KRLabsOrg/verbatim-rag-modern-bert-v1\",\n",
    "    device=\"cpu\",  # Works on CPU!\n",
    ")\n",
    "\n",
    "rag_bert = VerbatimRAG(index, extractor=extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- Runs entirely on CPU (no GPU needed)\n",
    "- No API costs\n",
    "- Fast inference (milliseconds)\n",
    "- Works offline\n",
    "\n",
    "Cons:\n",
    "- Requires model download\n",
    "- Less flexible than LLM\n",
    "\n",
    "**Note:** With ModernBERT extractor + SPLADE embeddings, the **entire pipeline runs on CPU** with no LLM calls!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: VerbatimIndex - Making Documents Searchable\n",
    "\n",
    "Before we can extract verbatim answers, we need to find relevant documents. The storage layer handles this.\n",
    "\n",
    "### Component 1: Document processing\n",
    "\n",
    "\n",
    "Before we chunk, we need to get documents into our system. This is where **DocumentSchema** comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 13:26:10,930 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-11-06 13:26:10,954 - INFO - Going to convert document batch...\n",
      "2025-11-06 13:26:10,959 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e647edf348883bed75367b22fbe60347\n",
      "2025-11-06 13:26:10,968 - INFO - Accelerator device: 'mps'\n",
      "2025-11-06 13:26:16,594 - INFO - Accelerator device: 'mps'\n",
      "2025-11-06 13:26:18,123 - INFO - Accelerator device: 'mps'\n",
      "2025-11-06 13:26:18,732 - INFO - Processing document 2025.bionlp-share.8.pdf\n",
      "2025-11-06 13:26:32,829 - INFO - Finished converting document 2025.bionlp-share.8.pdf in 22.90 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: KR Labs at ArchEHR-QA 2025\n",
      "Content length: 25,220 chars\n",
      "Content preview:\n",
      "## KR Labs at ArchEHR-QA 2025: A Verbatim Approach for Evidence-Based Question Answering\n",
      "\n",
      "Ádám Kovács 1 , Paul Schmitt 2 , Gábor Recski 1 , 2\n",
      "\n",
      "1 KR Labs lastname@krlabs.eu\n",
      "\n",
      "2 TU Wien firstname.lastnam...\n"
     ]
    }
   ],
   "source": [
    "from verbatim_rag.schema import DocumentSchema\n",
    "\n",
    "# From a PDF URL (uses Docling to convert PDF → Markdown)\n",
    "doc = DocumentSchema.from_url(\n",
    "    url=\"https://aclanthology.org/2025.bionlp-share.8.pdf\",\n",
    "    title=\"KR Labs at ArchEHR-QA 2025\",\n",
    "    authors=[\"Adam Kovacs\", \"Paul Schmitt\", \"Gabor Recski\"],\n",
    "    year=2025,\n",
    "    conference=\"BioNLP\",\n",
    "    category=\"nlp\",\n",
    ")\n",
    "\n",
    "print(f\"Document: {doc.title}\")\n",
    "print(f\"Content length: {len(doc.content):,} chars\")\n",
    "print(f\"Content preview:\\n{doc.content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What just happened?**\n",
    "\n",
    "1. Docling downloaded the PDF\n",
    "2. Converted it to clean markdown (preserving structure!)\n",
    "3. Created a DocumentSchema with your metadata\n",
    "4. All custom fields (`conference`, `category`) are preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create manually\n",
    "# If you already have the content\n",
    "doc = DocumentSchema(\n",
    "    content=\"# My Document\\n\\nContent here...\",\n",
    "    title=\"Custom Document\",\n",
    "    user_id=\"user123\",  # Custom field!\n",
    "    dataset_id=\"project_a\",  # Another custom field!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Component 2: Smart Chunking\n",
    "\n",
    "Most RAG systems chunk naively by character count. This loses critical context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive chunking (100 char chunks):\n",
      "\n",
      "--- Chunk 0 ---\n",
      "# Introduction\n",
      "Machine learning has revolutionized...\n",
      "\n",
      "--- Chunk 1 ---\n",
      "lated Work\n",
      "Previous approaches used rule-based met...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "\n",
      "We propose a novel architecture that combines dee...\n",
      "\n",
      "Problems:\n",
      "- Cut headings in half\n",
      "- Lost document structure\n",
      "- No context about which section text belongs to\n"
     ]
    }
   ],
   "source": [
    "# Bad: Arbitrary fixed-size chunking\n",
    "def naive_chunk(text, size=100):\n",
    "    return [text[i : i + size] for i in range(0, len(text), size)]\n",
    "\n",
    "\n",
    "markdown = \"\"\"# Introduction\n",
    "Machine learning has revolutionized AI by enabling systems to learn from data.\n",
    "\n",
    "## Related Work\n",
    "Previous approaches used rule-based methods which were limited in scope.\n",
    "\n",
    "## Our Approach\n",
    "We propose a novel architecture that combines deep learning with symbolic reasoning.\n",
    "\"\"\"\n",
    "\n",
    "chunks = naive_chunk(markdown, size=100)\n",
    "print(\"Naive chunking (100 char chunks):\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\n--- Chunk {i} ---\")\n",
    "    print(chunk[:50] + \"...\" if len(chunk) > 50 else chunk)\n",
    "\n",
    "print(\"\\nProblems:\")\n",
    "print(\"- Cut headings in half\")\n",
    "print(\"- Lost document structure\")\n",
    "print(\"- No context about which section text belongs to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbatim's Structure-Aware Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure-aware chunking:\n",
      "\n",
      "--- Chunk 0 ---\n",
      "Raw (for display):\n",
      "# Introduction\n",
      "Machine learning has revolutionized AI by enabling systems to lea...\n",
      "\n",
      "Enhanced (for embedding):\n",
      "# Introduction\n",
      "Machine learning has revolutionized AI by enabling systems to learn from data.\n",
      "\n",
      "\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Raw (for display):\n",
      "## Related Work\n",
      "Previous approaches used rule-based methods which were limited i...\n",
      "\n",
      "Enhanced (for embedding):\n",
      "# Introduction\n",
      "\n",
      "## Related Work\n",
      "Previous approaches used rule-based methods which were limited in sc...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Raw (for display):\n",
      "## Our Approach\n",
      "We propose a novel architecture that combines deep learning with...\n",
      "\n",
      "Enhanced (for embedding):\n",
      "# Introduction\n",
      "\n",
      "## Our Approach\n",
      "We propose a novel architecture that combines deep learning with sym...\n"
     ]
    }
   ],
   "source": [
    "from verbatim_rag.chunker_providers import MarkdownChunkerProvider\n",
    "\n",
    "chunker = MarkdownChunkerProvider(\n",
    "    split_levels=(1, 2, 3, 4),  # Split on H1, H2, H3, H4\n",
    "    include_preamble=True,  # Include text before first heading\n",
    ")\n",
    "\n",
    "chunks = chunker.chunk(markdown)\n",
    "\n",
    "print(\"Structure-aware chunking:\")\n",
    "for i, (raw, enhanced) in enumerate(chunks):\n",
    "    print(f\"\\n--- Chunk {i} ---\")\n",
    "    print(\"Raw (for display):\")\n",
    "    print(raw[:80] + \"...\" if len(raw) > 80 else raw)\n",
    "    print(\"\\nEnhanced (for embedding):\")\n",
    "    print(enhanced[:100] + \"...\" if len(enhanced) > 100 else enhanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Innovation:** Two versions of each chunk\n",
    "\n",
    "1. **Raw chunk:** Original text (used for display/extraction)\n",
    "2. **Enhanced chunk:** Original + ancestor headings (used for embedding/retrieval)\n",
    "\n",
    "**Why this matters:** When someone searches for \"novel approaches\", the enhanced chunk matches better because it has full context:\n",
    "- \"Introduction\" tells the model this is introductory content\n",
    "- \"Our Approach\" tells the model this is about the contribution\n",
    "\n",
    "### Component 2: Embeddings\n",
    "\n",
    "Embeddings convert text to vectors for semantic search.\n",
    "\n",
    "**Sparse Embeddings (SPLADE):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 13:17:52,171 - INFO - Load pretrained SparseEncoder: naver/splade-v3\n",
      "2025-11-06 13:17:54,365 - INFO - Loaded SPLADE model: naver/splade-v3\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector type: <class 'dict'>\n",
      "Non-zero dimensions: 32\n",
      "\n",
      "Top 5 terms:\n",
      "machine: 2.419\n",
      "training: 2.083\n",
      "learning: 1.881\n",
      "data: 1.821\n",
      "train: 1.613\n"
     ]
    }
   ],
   "source": [
    "from verbatim_rag.embedding_providers import SpladeProvider\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"naver/splade-v3\")\n",
    "\n",
    "embedder = SpladeProvider(\n",
    "    model_name=\"naver/splade-v3\",\n",
    "    device=\"cpu\",  # Works on CPU!\n",
    ")\n",
    "\n",
    "text = \"Machine learning models require training data\"\n",
    "vector = embedder.embed_text(text)\n",
    "\n",
    "print(f\"Vector type: {type(vector)}\")\n",
    "print(f\"Non-zero dimensions: {len(vector)}\")\n",
    "print(\"\\nTop 5 terms:\")\n",
    "for idx, weight in sorted(vector.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    token = tokenizer.convert_ids_to_tokens([idx])[0]\n",
    "    print(f\"{token}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dense Embeddings:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 11:50:34,103 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2025-11-06 11:50:36,346 - INFO - Loaded SentenceTransformers model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3e93e9e24144398f398cd66c03ad02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense vector length: 384\n",
      "Dense vector: All 384 dimensions have values\n"
     ]
    }
   ],
   "source": [
    "from verbatim_rag.embedding_providers import SentenceTransformersProvider\n",
    "\n",
    "embedder_dense = SentenceTransformersProvider(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "vector_dense = embedder_dense.embed_text(text)\n",
    "print(f\"Dense vector length: {len(vector_dense)}\")\n",
    "print(\"Dense vector: All 384 dimensions have values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Guide: Sparse vs Dense\n",
    "\n",
    "| Factor | Sparse (SPLADE) | Dense (SentenceTransformers) |\n",
    "|--------|-----------------|------------------------------|\n",
    "| **Hardware** | Fast on CPU | Faster with GPU |\n",
    "| **Accuracy** | Great for exact matches | Great for semantic similarity |\n",
    "| **Interpretability** | Higher (see which terms match) | Low (black box) |\n",
    "| **Model size** | ~100MB | ~100-500MB |\n",
    "| **Use case** | Precise matches | Generic text |\n",
    "\n",
    "**Recommendation:**\n",
    "- Start with SPLADE (sparse) for CPU-only deployments\n",
    "- Use dense if you have GPU and need max semantic similarity\n",
    "- Use both (hybrid) for best of both worlds\n",
    "\n",
    "### Component 3: Vector Stores\n",
    "\n",
    "Vector stores handle efficient storage and retrieval.\n",
    "\n",
    "**LocalMilvusStore:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verbatim_rag.vector_stores import LocalMilvusStore\n",
    "\n",
    "store_local = LocalMilvusStore(\n",
    "    db_path=\"./my_index.db\",\n",
    "    collection_name=\"my_collection\",\n",
    "    enable_sparse=True,\n",
    "    enable_dense=False,\n",
    ")\n",
    "\n",
    "print(\"LocalMilvusStore:\")\n",
    "print(\"- Stores data locally in a file\")\n",
    "print(\"- No cloud/network needed\")\n",
    "print(\"- Great for development and small datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CloudMilvusStore** (for production):\n",
    "\n",
    "```python\n",
    "from verbatim_rag.vector_stores import CloudMilvusStore\n",
    "\n",
    "store_cloud = CloudMilvusStore(\n",
    "    uri=\"https://your-milvus-instance.com\",\n",
    "    token=\"your-token\",\n",
    "    collection_name=\"production_collection\"\n",
    ")\n",
    "```\n",
    "\n",
    "Use for:\n",
    "- Production deployments\n",
    "- Large-scale datasets\n",
    "- Multi-tenant systems\n",
    "- High availability requirements\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Building Your Index Step-by-Step\n",
    "\n",
    "Now let's build the storage layer and add real research papers.\n",
    "\n",
    "### Step 1: Create DocumentSchemas from PDFs\n",
    "\n",
    "We'll add 6 research papers from ACL Anthology. Docling automatically converts PDFs to clean markdown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verbatim_rag.schema import DocumentSchema\n",
    "\n",
    "# Add 6 research papers from ACL Anthology\n",
    "# These papers demonstrate Verbatim RAG working with real academic literature\n",
    "\n",
    "paper = DocumentSchema.from_url(\n",
    "    url=\"https://aclanthology.org/L16-1417.pdf\",\n",
    "    title=\"Building Concept Graphs from Monolingual Dictionary Entries\",\n",
    "    doc_type=\"academic_paper\",\n",
    "    authors=[\"Gabor Recski\"],\n",
    "    conference=\"LREC\",\n",
    "    year=2016,\n",
    "    category=\"nlp\",\n",
    "    dataset_id=\"anthology\",\n",
    ")\n",
    "\n",
    "paper2 = DocumentSchema.from_url(\n",
    "    url=\"https://aclanthology.org/2025.bionlp-share.8.pdf\",\n",
    "    title=\"KR Labs at ArchEHR-QA 2025: A Verbatim Approach for Evidence-Based Question Answering\",\n",
    "    doc_type=\"academic_paper\",\n",
    "    authors=[\"Adam Kovacs\", \"Paul Schmitt\", \"Gabor Recski\"],\n",
    "    conference=\"BioNLP\",\n",
    "    year=2025,\n",
    "    category=\"nlp\",\n",
    "    dataset_id=\"anthology\",\n",
    ")\n",
    "\n",
    "paper3 = DocumentSchema.from_url(\n",
    "    url=\"https://aclanthology.org/2020.lrec-1.448.pdf\",\n",
    "    title=\"Better Together: Modern Methods Plus Traditional Thinking in NP Alignment\",\n",
    "    doc_type=\"academic_paper\",\n",
    "    authors=[\"Adam Kovacs\", \"Judit Acs\", \"Andras Kornai\", \"Gabor Recski\"],\n",
    "    conference=\"LREC\",\n",
    "    year=2020,\n",
    "    category=\"nlp\",\n",
    "    dataset_id=\"anthology\",\n",
    ")\n",
    "\n",
    "paper4 = DocumentSchema.from_url(\n",
    "    url=\"https://aclanthology.org/2020.msr-1.2.pdf\",\n",
    "    title=\"BME-TUW at SR'20: Lexical grammar induction for surface realization\",\n",
    "    doc_type=\"academic_paper\",\n",
    "    authors=[\n",
    "        \"Gábor Recski\",\n",
    "        \"Ádám Kovács\",\n",
    "        \"Kinga Gémes\",\n",
    "        \"Judit Ács\",\n",
    "        \"Andras Kornai\",\n",
    "    ],\n",
    "    conference=\"MSR\",\n",
    "    year=2020,\n",
    "    category=\"nlp\",\n",
    "    dataset_id=\"anthology\",\n",
    ")\n",
    "\n",
    "paper5 = DocumentSchema.from_url(\n",
    "    url=\"https://aclanthology.org/D19-6304.pdf\",\n",
    "    title=\"BME-UW at SRST-2019: Surface realization with Interpreted Regular Tree Grammars\",\n",
    "    doc_type=\"academic_paper\",\n",
    "    authors=[\"Ádám Kovács\", \"Evelin Ács\", \"Judit Ács\", \"Andras Kornai\", \"Gábor Recski\"],\n",
    "    conference=\"SRST\",\n",
    "    year=2019,\n",
    "    category=\"nlp\",\n",
    "    dataset_id=\"anthology\",\n",
    ")\n",
    "\n",
    "paper6 = DocumentSchema.from_url(\n",
    "    url=\"https://aclanthology.org/2020.semeval-1.15.pdf\",\n",
    "    title=\"BMEAUT at SemEval-2020 Task 2: Lexical entailment with semantic graphs\",\n",
    "    doc_type=\"academic_paper\",\n",
    "    authors=[\"Ádám Kovács\", \"Kinga Gémes\", \"Andras Kornai\", \"Gábor Recski\"],\n",
    "    conference=\"SemEval\",\n",
    "    year=2020,\n",
    "    category=\"nlp\",\n",
    "    dataset_id=\"anthology\",\n",
    ")\n",
    "\n",
    "print(\"Created 6 research papers:\")\n",
    "print(f\"  1. {paper.title} ({paper.metadata['year']})\")\n",
    "print(f\"  2. {paper2.title} ({paper2.metadata['year']})\")\n",
    "print(f\"  3. {paper3.title} ({paper3.metadata['year']})\")\n",
    "print(f\"  4. {paper4.title} ({paper4.metadata['year']})\")\n",
    "print(f\"  5. {paper5.title} ({paper5.metadata['year']})\")\n",
    "print(f\"  6. {paper6.title} ({paper6.metadata['year']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened:**\n",
    "1. Docling downloaded each PDF\n",
    "2. Converted them to clean markdown (preserving structure)\n",
    "3. Created DocumentSchemas with metadata\n",
    "4. All custom fields (conference, category, dataset_id) are preserved\n",
    "\n",
    "**Note:** All custom fields flow through the pipeline. You can filter by them later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Configure Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025-11-06 11:52:56,779 - INFO - Created indexes for collection: research_papers\n",
      "2025-11-06 11:52:56,785 - INFO - Created documents collection: research_papers_documents\n",
      "2025-11-06 11:52:56,786 - INFO - Connected to Milvus Lite: ./my_papers.db\n",
      "2025-11-06 11:52:56,797 - INFO - Load pretrained SparseEncoder: naver/splade-v3\n",
      "2025-11-06 11:52:58,907 - INFO - Loaded SPLADE model: naver/splade-v3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components configured!\n",
      "  Vector store: LocalMilvusStore\n",
      "  Embedder: SpladeProvider\n",
      "  Chunker: MarkdownChunkerProvider\n"
     ]
    }
   ],
   "source": [
    "from verbatim_rag.vector_stores import LocalMilvusStore\n",
    "from verbatim_rag.embedding_providers import SpladeProvider\n",
    "from verbatim_rag.chunker_providers import MarkdownChunkerProvider\n",
    "\n",
    "# Create vector store\n",
    "store = LocalMilvusStore(\n",
    "    db_path=\"./my_papers.db\",\n",
    "    collection_name=\"research_papers\",\n",
    "    enable_sparse=True,\n",
    "    enable_dense=False,\n",
    ")\n",
    "\n",
    "# Create embedder\n",
    "embedder = SpladeProvider(model_name=\"naver/splade-v3\", device=\"cpu\")\n",
    "\n",
    "# Create chunker\n",
    "chunker = MarkdownChunkerProvider(split_levels=(1, 2, 3, 4), include_preamble=True)\n",
    "\n",
    "print(\"Components configured!\")\n",
    "print(f\"  Vector store: {type(store).__name__}\")\n",
    "print(f\"  Embedder: {type(embedder).__name__}\")\n",
    "print(f\"  Chunker: {type(chunker).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Assemble Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storage layer ready!\n"
     ]
    }
   ],
   "source": [
    "from verbatim_rag import VerbatimIndex\n",
    "\n",
    "# Assemble the storage layer\n",
    "index = VerbatimIndex(\n",
    "    vector_store=store, sparse_provider=embedder, chunker_provider=chunker\n",
    ")\n",
    "\n",
    "print(\"Storage layer ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Add All 6 Papers to the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.19s/it]]\n",
      "2025-11-06 11:53:15,911 - INFO - Added 13 vectors to Milvus\n",
      "2025-11-06 11:53:15,914 - INFO - Added 1 documents to Milvus\n",
      "Batches: 100%|██████████| 1/1 [00:04<00:00,  4.75s/it]7.11s/it]\n",
      "2025-11-06 11:53:26,409 - INFO - Added 19 vectors to Milvus\n",
      "2025-11-06 11:53:26,410 - INFO - Added 1 documents to Milvus\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.66s/it]9.10s/it]\n",
      "2025-11-06 11:53:33,089 - INFO - Added 13 vectors to Milvus\n",
      "2025-11-06 11:53:33,090 - INFO - Added 1 documents to Milvus\n",
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.67s/it]8.00s/it]\n",
      "2025-11-06 11:53:41,654 - INFO - Added 16 vectors to Milvus\n",
      "2025-11-06 11:53:41,655 - INFO - Added 1 documents to Milvus\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.44s/it]8.22s/it]\n",
      "2025-11-06 11:53:47,681 - INFO - Added 12 vectors to Milvus\n",
      "2025-11-06 11:53:47,682 - INFO - Added 1 documents to Milvus\n",
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.18s/it]7.43s/it]\n",
      "2025-11-06 11:53:55,464 - INFO - Added 15 vectors to Milvus\n",
      "2025-11-06 11:53:55,465 - INFO - Added 1 documents to Milvus\n",
      "Adding documents: 100%|██████████| 6/6 [00:46<00:00,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 6 papers indexed!\n",
      "Total documents: 6\n",
      "Total chunks: 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Add all 6 papers in a single batch\n",
    "index.add_documents([paper, paper2, paper3, paper4, paper5, paper6])\n",
    "\n",
    "print(\"All 6 papers indexed!\")\n",
    "print(f\"Total documents: {index.inspect()['total_documents']}\")\n",
    "print(f\"Total chunks: {index.inspect()['total_chunks']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Working With Your Index\n",
    "\n",
    "Once you've indexed documents, you should inspect and query your index.\n",
    "\n",
    "### Inspecting What You Built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 6\n",
      "Total chunks: 88\n",
      "Average chunks per doc: 14.7\n",
      "\n",
      "Document types:\n",
      "  academic_paper: 6\n",
      "\n",
      "Sample documents:\n",
      "  - BME-TUW at SR'20: Lexical grammar induction for surface realization (N/A)\n",
      "  - BME-UW at SRST-2019: Surface realization with Interpreted Regular Tree Grammars (N/A)\n",
      "  - KR Labs at ArchEHR-QA 2025: A Verbatim Approach for Evidence-Based Question Answering (N/A)\n"
     ]
    }
   ],
   "source": [
    "# Get overview statistics\n",
    "stats = index.inspect()\n",
    "\n",
    "print(f\"Total documents: {stats['total_documents']}\")\n",
    "print(f\"Total chunks: {stats['total_chunks']}\")\n",
    "print(f\"Average chunks per doc: {stats['total_chunks'] / stats['total_documents']:.1f}\")\n",
    "\n",
    "print(\"\\nDocument types:\")\n",
    "for doc_type, count in stats[\"doc_types\"].items():\n",
    "    print(f\"  {doc_type}: {count}\")\n",
    "\n",
    "print(\"\\nSample documents:\")\n",
    "for doc in stats[\"sample_documents\"][:3]:\n",
    "    print(f\"  - {doc['title']} ({doc.get('year', 'N/A')})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Individual Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first 5 chunks\n",
    "chunks = index.get_all_chunks(limit=5)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Chunk {i}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    print(f\"\\nID: {chunk.id}\")\n",
    "    print(f\"Score: {chunk.score}\")\n",
    "\n",
    "    print(\"\\n--- Raw Text (for extraction/display) ---\")\n",
    "    print(chunk.text[:200] + \"...\")\n",
    "\n",
    "    print(\"\\n--- Enhanced Text (what was embedded) ---\")\n",
    "    print(chunk.enhanced_text[:300] + \"...\")\n",
    "\n",
    "    print(\"\\n--- Metadata ---\")\n",
    "    metadata_keys = [\"title\", \"authors\", \"year\", \"conference\", \"chunk_number\"]\n",
    "    for key in metadata_keys:\n",
    "        if key in chunk.metadata:\n",
    "            print(f\"  {key}: {chunk.metadata[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This shows you:**\n",
    "- How your chunks look after processing\n",
    "- What metadata was preserved\n",
    "- The difference between raw and enhanced text\n",
    "\n",
    "**Warning:** Not inspecting chunks is like deploying code without testing. Always check a few chunks!\n",
    "\n",
    "### Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for information about 4lang (mentioned in the 2016 paper)\n",
    "results = index.query(text=\"what is 4lang\", k=5)\n",
    "\n",
    "print(\"Top 5 results for '4lang':\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. {result.metadata['title']}\")\n",
    "    print(f\"   Score: {result.score:.3f}\")\n",
    "    print(f\"   Year: {result.metadata.get('year', 'N/A')}\")\n",
    "    print(f\"   Preview: {result.text[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering by Metadata\n",
    "\n",
    "Remember those custom fields? Now you can filter by them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only 2025 papers\n",
    "chunks_2025 = index.query(\n",
    "    text=None,  # No semantic search, just filter\n",
    "    filter='metadata[\"year\"] == 2025',\n",
    "    k=100,\n",
    ")\n",
    "print(f\"Found {len(chunks_2025)} chunks from 2025\")\n",
    "\n",
    "# Get papers from specific conference\n",
    "chunks_bionlp = index.query(filter='metadata[\"conference\"] == \"BioNLP\"', k=50)\n",
    "print(f\"Found {len(chunks_bionlp)} chunks from BioNLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Search + Filters\n",
    "\n",
    "The real power comes from combining semantic search with metadata filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 13.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top results for 'neural networks' (2024+):\n",
      "\n",
      "1. KR Labs at ArchEHR-QA 2025: A Verbatim Approach for Evidence-Based Question Answering\n",
      "   Score: 9.237\n",
      "   Year: 2025\n",
      "\n",
      "2. KR Labs at ArchEHR-QA 2025: A Verbatim Approach for Evidence-Based Question Answering\n",
      "   Score: 8.142\n",
      "   Year: 2025\n",
      "\n",
      "3. KR Labs at ArchEHR-QA 2025: A Verbatim Approach for Evidence-Based Question Answering\n",
      "   Score: 2.726\n",
      "   Year: 2025\n"
     ]
    }
   ],
   "source": [
    "# Search for \"neural networks\" in papers from 2024 onwards\n",
    "results = index.query(\n",
    "    text=\"neural network architectures\", filter='metadata[\"year\"] >= 2024', k=10\n",
    ")\n",
    "\n",
    "print(\"Top results for 'neural networks' (2024+):\")\n",
    "for i, result in enumerate(results[:3], 1):\n",
    "    print(f\"\\n{i}. {result.metadata['title']}\")\n",
    "    print(f\"   Score: {result.score:.3f}\")\n",
    "    print(f\"   Year: {result.metadata.get('year', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much more precise than pure semantic search!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Integration Patterns - Wrapping Your Existing RAG\n",
    "\n",
    "You don't have to rebuild everything. Verbatim RAG can wrap your existing retrieval system.\n",
    "\n",
    "### The RAGProvider Interface\n",
    "\n",
    "To integrate with existing systems, implement this simple interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verbatim_rag.providers import RAGProvider\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "\n",
    "class MyCustomRAGProvider(RAGProvider):\n",
    "    \"\"\"Wrap your existing RAG system.\"\"\"\n",
    "\n",
    "    def retrieve(\n",
    "        self, question: str, k: int = 5, filter: Optional[str] = None\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Return context as list of dicts.\n",
    "\n",
    "        Must return:\n",
    "        [\n",
    "            {\n",
    "                \"content\": str (required),\n",
    "                \"title\": str (optional),\n",
    "                \"source\": str (optional),\n",
    "                \"metadata\": dict (optional)\n",
    "            },\n",
    "            ...\n",
    "        ]\n",
    "        \"\"\"\n",
    "        # Your retrieval logic here\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Just one method that returns a list of dicts with:\n",
    "- `content` (required): The text content\n",
    "- `title` (optional): Document title\n",
    "- `source` (optional): Document source/URL\n",
    "- `metadata` (optional): Any additional metadata\n",
    "\n",
    "### Example: Wrapping LangChain\n",
    "\n",
    "Here's a complete working example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain-openai langchain-community openai faiss-cpu langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 13:23:43,998 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 13:23:44,916 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting spans (batch mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 13:23:47,236 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 13:23:49,234 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What methods were used in the study? Here are the key approaches:\n",
      "\n",
      "1. **Fact 1:** [1] We used two approaches: zero-shot LLM extraction and a fine-tuned ModernBERT classifier on 58k synthetic examples.\n"
     ]
    }
   ],
   "source": [
    "from verbatim_rag.providers import RAGProvider\n",
    "from verbatim_rag import verbatim_query\n",
    "\n",
    "# Your existing LangChain setup\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# 1. Create sample documents\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "# Methods\n",
    "We used two approaches: zero-shot LLM extraction and a fine-tuned ModernBERT classifier on 58k synthetic examples.\n",
    "\n",
    "# Results\n",
    "The system achieved 42.01% accuracy on ArchEHR-QA.\n",
    "        \"\"\",\n",
    "        metadata={\"title\": \"Research Paper 1\", \"source\": \"paper1.pdf\", \"year\": 2025},\n",
    "    )\n",
    "]\n",
    "\n",
    "# 2. Index with LangChain (your existing setup)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "\n",
    "# 3. Wrap your LangChain retriever\n",
    "class LangChainRAGProvider(RAGProvider):\n",
    "    def __init__(self, langchain_retriever):\n",
    "        self.retriever = langchain_retriever\n",
    "\n",
    "    def retrieve(\n",
    "        self, question: str, k: int = 5, filter: Optional[str] = None\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        # Use LangChain's retrieval\n",
    "        docs = self.retriever.invoke(question)\n",
    "\n",
    "        # Convert to Verbatim format\n",
    "        context = []\n",
    "        for doc in docs[:k]:\n",
    "            context.append(\n",
    "                {\n",
    "                    \"content\": doc.page_content,\n",
    "                    \"title\": doc.metadata.get(\"title\", \"\"),\n",
    "                    \"source\": doc.metadata.get(\"source\", \"\"),\n",
    "                    \"metadata\": doc.metadata,\n",
    "                }\n",
    "            )\n",
    "        return context\n",
    "\n",
    "\n",
    "# 4. Use Verbatim with your existing LangChain RAG\n",
    "provider = LangChainRAGProvider(retriever)\n",
    "response = verbatim_query(provider, \"What methods were used?\", k=5)\n",
    "\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What you get:**\n",
    "- Keep your existing LangChain + FAISS setup\n",
    "- Keep your OpenAI embeddings\n",
    "- Add hallucination-free span extraction\n",
    "- Get precise, verifiable citations\n",
    "\n",
    "**Zero changes** to your current indexing pipeline!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Standalone Components\n",
    "\n",
    "You can also use individual components without the full system.\n",
    "\n",
    "### VerbatimTransform (Most Common)\n",
    "\n",
    "The core transformation component that takes ANY context and produces verbatim answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting spans (batch mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 13:23:58,767 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 13:24:00,563 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of the evaluation are as follows:\n",
      "\n",
      "1. **Fact 1:** [1] Our system achieved 42.01% accuracy on ArchEHR-QA.\n",
      "2. **Fact 2:** [2] Baselines achieved 35.4% on the same benchmark.\n",
      "\n",
      "Extracted spans:\n",
      "  - Our system achieved 42.01% accuracy on ArchEHR-QA.\n",
      "  - Baselines achieved 35.4% on the same benchmark.\n"
     ]
    }
   ],
   "source": [
    "from verbatim_rag import VerbatimTransform\n",
    "\n",
    "# Your retrieval (from anywhere!)\n",
    "context = [\n",
    "    {\"content\": \"Our system achieved 42.01% accuracy on ArchEHR-QA.\"},\n",
    "    {\"content\": \"Baselines achieved 35.4% on the same benchmark.\"},\n",
    "]\n",
    "\n",
    "# Extract and compose verbatim answer\n",
    "vt = VerbatimTransform()\n",
    "response = vt.transform(question=\"What were the results?\", context=context)\n",
    "\n",
    "print(response.answer)\n",
    "\n",
    "# Inspect extracted spans\n",
    "print(\"\\nExtracted spans:\")\n",
    "for doc in response.documents:\n",
    "    for h in doc.highlights:\n",
    "        print(f\"  - {h.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use case:** You have retrieval working, just need extraction and composition.\n",
    "\n",
    "### LLMSpanExtractor\n",
    "\n",
    "Just want span extraction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting spans (batch mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 13:24:04,801 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Our system achieved 42.01% accuracy.\n"
     ]
    }
   ],
   "source": [
    "from verbatim_rag.extractors import LLMSpanExtractor\n",
    "from verbatim_rag.vector_stores import SearchResult\n",
    "\n",
    "# Your chunks (from any retrieval system)\n",
    "chunks = [\n",
    "    SearchResult(\n",
    "        id=\"1\", score=0.95, text=\"Our system achieved 42.01% accuracy.\", metadata={}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Extract ONLY the relevant spans\n",
    "extractor = LLMSpanExtractor(model=\"gpt-4o-mini\")\n",
    "relevant_spans = extractor.extract_spans(\"What is the accuracy?\", chunks)\n",
    "\n",
    "# relevant_spans is a dict: {document_text: [span1, span2, ...]}\n",
    "for doc_text, spans in relevant_spans.items():\n",
    "    for span in spans:\n",
    "        if span.strip():\n",
    "            print(f\"  - {span}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use case:** You just need the extraction logic, will handle composition yourself.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Advanced Configuration\n",
    "\n",
    "### Custom Answer Templates\n",
    "\n",
    "Want full control over answer formatting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your custom template\n",
    "template = \"\"\"\n",
    "Thanks for your question!\n",
    "\n",
    "You can find the answer in the following sources:\n",
    "[RELEVANT_SENTENCES]\n",
    "\n",
    "Please let me know if you need more details.\n",
    "\"\"\"\n",
    "\n",
    "# Use static mode\n",
    "rag.template_manager.use_static_mode(template)\n",
    "\n",
    "response = rag.query(\"What methods were used?\")\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `[RELEVANT_SENTENCES]` placeholder is automatically replaced with numbered citations.\n",
    "\n",
    "**When to use:**\n",
    "- Static mode: Consistent formatting (customer support, reports)\n",
    "- Contextual mode (default): LLM adapts template to each question\n",
    "\n",
    "### Use Your Own LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verbatim_rag.core import LLMClient\n",
    "\n",
    "# Configure any OpenAI-compatible endpoint\n",
    "llm_client = LLMClient(\n",
    "    model=\"anthropic/claude-3-5-sonnet\",\n",
    "    api_base=\"https://api.anthropic.com/v1\",\n",
    ")\n",
    "\n",
    "rag_claude = VerbatimRAG(index, llm_client=llm_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid Search (Sparse + Dense)\n",
    "\n",
    "Use both embedding types for best accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from verbatim_rag.embedding_providers import (\n",
    "    SpladeProvider,\n",
    "    SentenceTransformersProvider,\n",
    ")\n",
    "\n",
    "sparse = SpladeProvider(\"naver/splade-v3\", device=\"cpu\")\n",
    "dense = SentenceTransformersProvider(\"all-MiniLM-L6-v2\", device=\"cpu\")\n",
    "\n",
    "store = LocalMilvusStore(\n",
    "    \"./hybrid.db\",\n",
    "    enable_sparse=True,\n",
    "    enable_dense=True,\n",
    "    dense_dim=384,  # Dimension of all-MiniLM-L6-v2\n",
    ")\n",
    "\n",
    "index = VerbatimIndex(vector_store=store, sparse_provider=sparse, dense_provider=dense)\n",
    "\n",
    "# Now retrieval uses both sparse and dense embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**1. The Problem:**\n",
    "- Traditional RAG systems hallucinate even with perfect retrieval\n",
    "- LLMs round numbers, paraphrase terms, combine facts incorrectly\n",
    "\n",
    "**2. The Solution:**\n",
    "- Verbatim RAG extracts exact text spans (no generation)\n",
    "- Composes answers from verbatim quotes\n",
    "- Every claim is verifiable\n",
    "\n",
    "**3. The Architecture:**\n",
    "- Two independent layers: VerbatimIndex + Verbatim Core\n",
    "- Modular design: swap any component\n",
    "- Use parts or the whole system\n",
    "\n",
    "**4. Integration Options:**\n",
    "- Full system\n",
    "- Wrap existing RAG (LangChain, LlamaIndex)\n",
    "- Standalone components (just extraction)\n",
    "\n",
    "**5. Production Deployment:**\n",
    "- CPU-only option (SPLADE + ModernBERT)\n",
    "- Cloud storage (CloudMilvusStore)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Try it with your documents**\n",
    "2. **Experiment with different extractors** (LLM vs ModernBERT)\n",
    "3. **Integrate with your existing RAG** (if you have one)\n",
    "4. **Deploy to production** (use cloud storage)\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **GitHub**: [github.com/KRLabsOrg/verbatim-rag](https://github.com/KRLabsOrg/verbatim-rag)\n",
    "- **Research Paper**: [ACL Anthology 2025](https://aclanthology.org/2025.bionlp-share.8/)\n",
    "- **Models**: [HuggingFace](https://huggingface.co/KRLabsOrg)\n",
    "\n",
    "### Citation\n",
    "\n",
    "If you use Verbatim RAG in your research, please cite our paper:\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{kovacs-etal-2025-kr,\n",
    "    title = \"{KR} Labs at {A}rch{EHR}-{QA} 2025: A Verbatim Approach for Evidence-Based Question Answering\",\n",
    "    author = \"Kovacs, Adam and Schmitt, Paul and Recski, Gabor\",\n",
    "    booktitle = \"Proceedings of the 24th Workshop on Biomedical Language Processing\",\n",
    "    year = \"2025\",\n",
    "    url = \"https://aclanthology.org/2025.bionlp-share.8/\",\n",
    "    pages = \"69--74\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
