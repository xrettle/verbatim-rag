{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd22aabb-fa28-4f45-b736-f8f810e23ea0",
   "metadata": {},
   "source": [
    "# Build Verbatim: A Developer's Guide to hallucination free RAG\n",
    "\n",
    "*A hands-on tutorial for developers who want to build trustworthy, verifiable RAG systems*\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://github.com/KRLabsOrg/verbatim-rag/blob/main/assets/chiliground.png?raw=true\" alt=\"ChiliGround Logo\" width=\"400\"/>\n",
    "  <br><em>Chill, I Ground! üå∂ Ô∏è</em>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1eb375d-a0cd-485a-958c-61a041b7526f",
   "metadata": {},
   "source": [
    "## The Hallucination Problem\n",
    "\n",
    "If you've built RAG systems before, you've probably seen this:\n",
    "\n",
    "**Question:** \"How much synthetic data was generated?\"\n",
    "\n",
    "**Traditional RAG Answer:** \"They generated around 58,000 synthetic training examples...\"\n",
    "\n",
    "**The actual source:** \"We constructed a comprehensive dataset of 60k synthetic training examples...\"\n",
    "\n",
    "Notice the problem? The LLM rounded \"58k\" to \"around 60k\". In medical, legal, or financial domains, this approximation could be catastrophic.\n",
    "\n",
    "Even with retrieval-augmented generation, LLMs can still:\n",
    "- Round or approximate numbers\n",
    "- Combine information incorrectly\n",
    "- Fill in gaps with training knowledge\n",
    "- Generate plausible-sounding but false information\n",
    "\n",
    "**What if we could force the LLM to only use exact text from the source documents?**\n",
    "\n",
    "That's exactly what Verbatim RAG does.\n",
    "\n",
    "---\n",
    "\n",
    "## The Verbatim Solution\n",
    "\n",
    "Instead of letting the LLM freely generate text based on retrieved documents, Verbatim RAG:\n",
    "\n",
    "1. **Extracts exact text spans** from source documents\n",
    "2. **Composes answers** entirely from these verbatim passages\n",
    "3. **Provides precise citations** linking back to sources\n",
    "\n",
    "The result? Zero hallucinations. Every claim is directly traceable to the source text.\n",
    "\n",
    "But here's what makes Verbatim RAG special: **it's modular and RAG-agnostic**. You can use it as a complete RAG system, or just use its hallucination-prevention components in your existing pipeline.\n",
    "\n",
    "In this guide, we'll build a Verbatim RAG system from scratch, understanding each component along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec7c549-895c-4fd3-a6a5-7c6e4d295ea4",
   "metadata": {},
   "source": [
    "## Understanding the Architecture\n",
    "\n",
    "Verbatim RAG is modular - each step is configurable so you can pick what works best for your use case.\n",
    "\n",
    "Here's the complete pipeline:\n",
    "\n",
    "```\n",
    "Document (pdf, docx, csv, etc..) ‚Üí Markdown ‚Üí DocumentSchema ‚Üí Chunking ‚Üí Enhanced Chunks ‚Üí Embedding ‚Üí Vector Store ‚Üí Query ‚Üí Extract Spans ‚Üí Compose Answer\n",
    "```\n",
    "\n",
    "Let's unpack each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8281e927-d372-4b9f-b62a-9bc3ace4b5a5",
   "metadata": {},
   "source": [
    "## Part 0: Installation\n",
    "\n",
    "First, install Verbatim RAG and its dependencies:\n",
    "\n",
    "```bash\n",
    "pip install verbatim-rag\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84020b-5433-427e-a2a7-8e9df0602758",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: The Complete Picture (5-Minute Quickstart)\n",
    "\n",
    "Let's see Verbatim RAG in action, then understand how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f391e7e7-e5df-4585-bb48-5c9d1e836883",
   "metadata": {},
   "source": [
    "### Working Example\n",
    "\n",
    "Verbatim RAG is **fully modular** - you can configure every component of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1630d25-fbbf-4b36-ae23-1a869928ffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from verbatim_rag import VerbatimRAG, VerbatimIndex\n",
    "from verbatim_rag.vector_stores import LocalMilvusStore\n",
    "from verbatim_rag.embedding_providers import SpladeProvider\n",
    "from verbatim_rag.schema import DocumentSchema\n",
    "\n",
    "# Create index (storage layer)\n",
    "store = LocalMilvusStore(\"./demo.db\", enable_sparse=True, enable_dense=False)\n",
    "embedder = SpladeProvider(\"naver/splade-v3\", device=\"cpu\")\n",
    "index = VerbatimIndex(vector_store=store, sparse_provider=embedder)\n",
    "\n",
    "# Add a document\n",
    "doc = DocumentSchema(\n",
    "    content=\"\"\"\n",
    "# Methods\n",
    "We used two approaches: zero-shot LLM extraction and a fine-tuned ModernBERT classifier on 58k synthetic examples.\n",
    "    \n",
    "# Results\n",
    "The system achieved 42.01% accuracy on ArchEHR-QA.\n",
    "\"\"\",\n",
    "    title=\"Research Paper\",\n",
    ")\n",
    "index.add_documents([doc])\n",
    "\n",
    "# Create RAG (add intelligence layer)\n",
    "rag = VerbatimRAG(index, model=\"gpt-4o-mini\")\n",
    "\n",
    "# Ask questions ‚Üí get exact quotes\n",
    "response = rag.query(\"What approaches were used?\")\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4df45a-0b03-42c5-9f14-ffc6fa103b46",
   "metadata": {},
   "source": [
    "\n",
    "That's it! Now you have a complete RAG system with hallucination prevention. Each component is independent and swappable.\n",
    "\n",
    "### The Architecture: Two Layers\n",
    "\n",
    "**Storage Layer (VerbatimIndex):**\n",
    "1. **Chunker** - Splits documents into searchable pieces\n",
    "2. **Embedder** - Converts text to vectors for semantic search (dense/sparse)\n",
    "3. **Vector Store** - Stores and retrieves your data (local/cloud)\n",
    "\n",
    "**Intelligence Layer (VerbatimRAG):**\n",
    "1. **Span Extractor** - Identifies exact text that answers questions (LLM/model-based)\n",
    "2. **Template Manager** - Structures answers from extracted spans (static/contextual)\n",
    "3. **LLM Client** - Powers extraction and composition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c47b091-f902-4306-b539-3e44665ee344",
   "metadata": {},
   "source": [
    "Roadmap: How We'll Build This\n",
    "\n",
    "- **Parts 2-4:** Configure storage components (chunker, embedder, vector store)\n",
    "- **Part 5:** Assemble storage layer (`VerbatimIndex`) - now you can index & retrieve\n",
    "- **Parts 6-7:** Use the storage layer (inspect chunks, semantic search)\n",
    "- **Part 8:** Add intelligence layer (`VerbatimRAG`) - now you get hallucination-free answers\n",
    "- **Parts 9-11:** Integration patterns and complete examples\n",
    "\n",
    "By the end, you'll understand every component and how to customize them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd1d186-2fd6-43cc-9a08-ef161a065686",
   "metadata": {},
   "source": [
    "### Choose Your Path\n",
    "\n",
    "**Already have a RAG system?**\n",
    "‚Üí Skip to [Part 9](#part-9-integration-patterns---wrapping-existing-rag) - Wrap your LangChain/LlamaIndex/custom RAG\n",
    "\n",
    "**Just want extraction for your existing retrieval?**\n",
    "‚Üí Skip to [Part 10](#part-10-using-components-standalone) - Use VerbatimTransform standalone\n",
    "\n",
    "**Want to see a full example first?**\n",
    "‚Üí Jump to [Part 11](#part-11-building-a-complete-system) - Research paper search engine\n",
    "\n",
    "**Ready to learn each component?**\n",
    "‚Üí Continue to Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef64910-8934-4e4f-a10f-c5a0e0e694a6",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2: Chunking - The Foundation\n",
    "\n",
    "Chunking is where most RAG systems make or break. Bad chunking ‚Üí bad retrieval ‚Üí bad answers.\n",
    "\n",
    "### Why Chunking Matters\n",
    "\n",
    "Let's say you have a research paper with this structure:\n",
    "\n",
    "```markdown\n",
    "# Introduction\n",
    "Machine learning has revolutionized AI...\n",
    "\n",
    "## Related Work\n",
    "Previous approaches used rule-based methods...\n",
    "\n",
    "## Our Approach\n",
    "We propose a novel architecture...\n",
    "```\n",
    "\n",
    "If you chunk this into arbitrary 512-character blocks, you might get:\n",
    "\n",
    "```\n",
    "Chunk 1: \"...has revolutionized AI. Previous approaches used rule-based...\"\n",
    "```\n",
    "\n",
    "**Problem:** This chunk lost all context! Which section is it from? What's the relationship between these sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29887b3c-4e76-43c8-9d8e-7c4e253d886f",
   "metadata": {},
   "source": [
    "### Enter: The Markdown Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bf857d3-d8c3-4bbb-8bb5-c5e741905e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from verbatim_rag.chunker_providers import MarkdownChunkerProvider\n",
    "\n",
    "chunker = MarkdownChunkerProvider(\n",
    "    split_levels=(1, 2, 3, 4),  # Split on H1, H2, H3, H4\n",
    "    include_preamble=True,  # Include text before first heading\n",
    ")\n",
    "\n",
    "markdown = \"\"\"# Introduction\n",
    "Machine learning has revolutionized AI.\n",
    "\n",
    "## Related Work\n",
    "Previous approaches used rule-based methods.\n",
    "\n",
    "## Our Approach\n",
    "We propose a novel architecture.\n",
    "\"\"\"\n",
    "\n",
    "chunks = chunker.chunk(markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e34d12e-d963-48a6-8d91-f9c46adc21b8",
   "metadata": {},
   "source": [
    "\n",
    "This chunker does something clever: it returns **two versions** of each chunk:\n",
    "\n",
    "1. **Raw chunk** - The original text (used for display/extraction)\n",
    "2. **Enhanced chunk** - Original text **plus ancestor headings** (used for embedding/retrieval)\n",
    "\n",
    "Let's see what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14c4c77f-a12a-4c72-912d-6e82cc1d1758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 0 ---\n",
      "Raw:\n",
      "# Introduction\n",
      "Machine learning has revolutionized AI.\n",
      "\n",
      "\n",
      "\n",
      "Enhanced:\n",
      "# Introduction\n",
      "Machine learning has revolutionized AI.\n",
      "\n",
      "\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Raw:\n",
      "## Related Work\n",
      "Previous approaches used rule-based methods.\n",
      "\n",
      "\n",
      "\n",
      "Enhanced:\n",
      "# Introduction\n",
      "\n",
      "## Related Work\n",
      "Previous approaches used rule-based methods.\n",
      "\n",
      "\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Raw:\n",
      "## Our Approach\n",
      "We propose a novel architecture.\n",
      "\n",
      "\n",
      "Enhanced:\n",
      "# Introduction\n",
      "\n",
      "## Our Approach\n",
      "We propose a novel architecture.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (raw, enhanced) in enumerate(chunks):\n",
    "    print(f\"\\n--- Chunk {i} ---\")\n",
    "    print(f\"Raw:\\n{raw}\\n\")\n",
    "    print(f\"Enhanced:\\n{enhanced}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2e201e-a81b-4b0d-9f6b-4519baf8a2b9",
   "metadata": {},
   "source": [
    "\n",
    "## Part 3: Document Processing\n",
    "\n",
    "Before we chunk, we need to get documents into our system. This is where **DocumentSchema** comes in.\n",
    "\n",
    "### Creating a DocumentSchema\n",
    "\n",
    "DocumentSchema is your structured document with metadata. It's like a typed dict, but better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9db6e39-5243-4f35-b97a-baa00f6e8a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 17:16:47,154 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-10-24 17:16:47,218 - INFO - Going to convert document batch...\n",
      "2025-10-24 17:16:47,219 - INFO - Initializing pipeline for StandardPdfPipeline with options hash e647edf348883bed75367b22fbe60347\n",
      "2025-10-24 17:16:47,248 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-10-24 17:16:47,250 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-10-24 17:16:47,256 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-10-24 17:16:47,260 - INFO - Registered ocr engines: ['easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-10-24 17:16:49,512 - INFO - Accelerator device: 'mps'\n",
      "2025-10-24 17:16:52,097 - INFO - Accelerator device: 'mps'\n",
      "2025-10-24 17:16:53,140 - INFO - Accelerator device: 'mps'\n",
      "2025-10-24 17:16:53,746 - INFO - Processing document 2025.bionlp-share.8.pdf\n",
      "2025-10-24 17:17:01,365 - INFO - Finished converting document 2025.bionlp-share.8.pdf in 15.19 sec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: KR Labs at ArchEHR-QA 2025\n",
      "Content length: 25,220 chars\n",
      "Content preview:\n",
      "## KR Labs at ArchEHR-QA 2025: A Verbatim Approach for Evidence-Based Question Answering\n",
      "\n",
      "√Åd√°m Kov√°cs 1 , Paul Schmitt 2 , G√°bor Recski 1 , 2\n",
      "\n",
      "1 KR Labs lastname@krlabs.eu\n",
      "\n",
      "2 TU Wien firstname.lastnam...\n"
     ]
    }
   ],
   "source": [
    "from verbatim_rag.schema import DocumentSchema\n",
    "\n",
    "# From a PDF URL (uses Docling to convert PDF ‚Üí Markdown)\n",
    "doc = DocumentSchema.from_url(\n",
    "    url=\"https://aclanthology.org/2025.bionlp-share.8.pdf\",\n",
    "    title=\"KR Labs at ArchEHR-QA 2025\",\n",
    "    authors=[\"Adam Kovacs\", \"Paul Schmitt\", \"Gabor Recski\"],\n",
    "    year=2025,\n",
    "    conference=\"BioNLP\",\n",
    "    category=\"nlp\",\n",
    ")\n",
    "\n",
    "print(f\"Document: {doc.title}\")\n",
    "print(f\"Content length: {len(doc.content):,} chars\")\n",
    "print(f\"Content preview:\\n{doc.content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea8e091-1b04-4789-b264-fc0be241a7a0",
   "metadata": {},
   "source": [
    "\n",
    "**What just happened?**\n",
    "\n",
    "1. Docling downloaded the PDF\n",
    "2. Converted it to clean markdown (preserving structure!)\n",
    "3. Created a DocumentSchema with your metadata\n",
    "4. All custom fields (`conference`, `category`) are preserved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d66617-4347-4dc9-8321-6dabf3f30ac7",
   "metadata": {},
   "source": [
    "### Manual Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7157d3fa-7e05-48eb-978e-214f5120ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you already have the content\n",
    "doc = DocumentSchema(\n",
    "    content=\"# My Document\\n\\nContent here...\",\n",
    "    title=\"Custom Document\",\n",
    "    user_id=\"user123\",  # Custom field!\n",
    "    dataset_id=\"project_a\",  # Another custom field!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079fb4d7-c168-449e-bb9a-50f2c977b2ca",
   "metadata": {},
   "source": [
    "**Key Insight:** All custom fields are preserved throughout the pipeline. When you query, you can filter by any of these fields!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d35817-d452-4317-9073-176509fb5878",
   "metadata": {},
   "source": [
    "## Part 4: Embeddings - Making Text Searchable\n",
    "\n",
    "Now we need to convert text into vectors so we can search by meaning, not just keywords.\n",
    "\n",
    "### Dense vs Sparse Embeddings\n",
    "\n",
    "**Dense embeddings**\n",
    "- Every dimension has a value\n",
    "- 384-1536 dimensions\n",
    "- Examples: `all-MiniLM-L6-v2`, OpenAI's `text-embedding-3-small`\n",
    "\n",
    "**Sparse embeddings**\n",
    "- Most dimensions are zero\n",
    "- Only important terms get non-zero weights\n",
    "- More interpretable!\n",
    "- Examples: SPLADE, BM25\n",
    "\n",
    "Let's try SPLADE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67c468e3-3770-4d3b-9cfc-195f5213c2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 17:46:04,290 - INFO - Load pretrained SparseEncoder: naver/splade-v3\n",
      "2025-10-24 17:46:06,914 - INFO - Loaded SPLADE model: naver/splade-v3\n",
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector type: <class 'dict'>\n",
      "Non-zero dimensions: 32\n",
      "\n",
      "Top 5 terms:\n",
      "  Dimension 3698: 2.419\n",
      "  Dimension 2731: 2.083\n",
      "  Dimension 4083: 1.881\n",
      "  Dimension 2951: 1.821\n",
      "  Dimension 3345: 1.613\n"
     ]
    }
   ],
   "source": [
    "from verbatim_rag.embedding_providers import SpladeProvider\n",
    "\n",
    "embedder = SpladeProvider(\n",
    "    model_name=\"naver/splade-v3\",\n",
    "    device=\"cpu\",  # Works on CPU!\n",
    ")\n",
    "\n",
    "# Embed a single text\n",
    "text = \"Machine learning models require training data\"\n",
    "vector = embedder.embed_text(text)\n",
    "\n",
    "print(f\"Vector type: {type(vector)}\")\n",
    "print(f\"Non-zero dimensions: {len(vector)}\")\n",
    "print(\"\\nTop 5 terms:\")\n",
    "for idx, weight in sorted(vector.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"  Dimension {idx}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7061815b-cf27-4c87-981b-6d28281ef837",
   "metadata": {},
   "source": [
    "\n",
    "### Dense Embeddings\n",
    "\n",
    "Want dense embeddings instead? Just swap the embedder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fd4e345-9290-432c-8e9f-c2207e709dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-24 17:46:56,319 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2025-10-24 17:46:59,749 - INFO - Loaded SentenceTransformers model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f24754e81c4223b5cad24543dbcb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense vector length: 384\n"
     ]
    }
   ],
   "source": [
    "from verbatim_rag.embedding_providers import SentenceTransformersProvider\n",
    "\n",
    "embedder = SentenceTransformersProvider(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\", device=\"cpu\"\n",
    ")\n",
    "\n",
    "vector = embedder.embed_text(text)\n",
    "print(f\"Dense vector length: {len(vector)}\")  # 384 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a2dd54-aa4c-4c85-a4db-a6ab23f7b19d",
   "metadata": {},
   "source": [
    "## Part 5: Putting It All Together\n",
    "\n",
    "In Part 1, we showed you the **complete VerbatimRAG system**. Now let's build it step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d89a5f-09f6-447d-ad2a-afe8d09c3fbc",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: Build the Storage Layer (VerbatimIndex)\n",
    "\n",
    "First, we assemble the index with our configured components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9bcc51e-ae96-4cf7-bae7-18585dd1a03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2025-10-24 17:47:40,752 - INFO - Created indexes for collection: research_papers\n",
      "2025-10-24 17:47:40,757 - INFO - Created documents collection: research_papers_documents\n",
      "2025-10-24 17:47:40,758 - INFO - Connected to Milvus Lite: ./my_papers.db\n",
      "2025-10-24 17:47:40,770 - INFO - Load pretrained SparseEncoder: naver/splade-v3\n",
      "2025-10-24 17:47:42,936 - INFO - Loaded SPLADE model: naver/splade-v3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Storage layer ready!\n"
     ]
    }
   ],
   "source": [
    "from verbatim_rag import VerbatimIndex\n",
    "from verbatim_rag.vector_stores import LocalMilvusStore\n",
    "from verbatim_rag.embedding_providers import SpladeProvider\n",
    "from verbatim_rag.chunker_providers import MarkdownChunkerProvider\n",
    "\n",
    "# Create each provider\n",
    "store = LocalMilvusStore(\n",
    "    db_path=\"./my_papers.db\",\n",
    "    collection_name=\"research_papers\",\n",
    "    enable_sparse=True,\n",
    "    enable_dense=False,\n",
    ")\n",
    "\n",
    "embedder = SpladeProvider(\"naver/splade-v3\", device=\"cpu\")\n",
    "chunker = MarkdownChunkerProvider()\n",
    "\n",
    "# Assemble the storage layer\n",
    "index = VerbatimIndex(\n",
    "    vector_store=store, sparse_provider=embedder, chunker_provider=chunker\n",
    ")\n",
    "\n",
    "print(\"‚úì Storage layer ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab8dd93-d498-4e29-9f9c-0f7336503d0b",
   "metadata": {},
   "source": [
    "### Step 2: Add the Intelligence Layer (VerbatimRAG)\n",
    "\n",
    "Now let's add hallucination prevention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d2d90c5-43f8-44ba-b040-7edfc3ab1b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Complete RAG system ready!\n"
     ]
    }
   ],
   "source": [
    "from verbatim_rag import VerbatimRAG\n",
    "\n",
    "# Build complete RAG system with extraction\n",
    "rag = VerbatimRAG(\n",
    "    index=index,\n",
    "    model=\"gpt-4o-mini\",  # For span extraction\n",
    "    max_display_spans=5,\n",
    ")\n",
    "\n",
    "print(\"‚úì Complete RAG system ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a102ff18-bace-44f9-ba4c-07ff931baa3a",
   "metadata": {},
   "source": [
    "\n",
    "Now you have:\n",
    "- **VerbatimIndex**: Storage layer (retrieves relevant documents)\n",
    "- **VerbatimRAG**: Complete system (retrieves + extracts verbatim spans)\n",
    "\n",
    "üí° **Key Difference:**\n",
    "- `index.query(\"question\")` ‚Üí Returns relevant chunks\n",
    "- `rag.query(\"question\")` ‚Üí Returns verbatim answer with citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da151e6-a93e-417c-9fe4-c1db8ffe411a",
   "metadata": {},
   "source": [
    "### Add some documents to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a2808-fc94-4395-8e69-b3107bf0c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from verbatim_rag import VerbatimIndex\n",
    "from verbatim_rag.schema import DocumentSchema\n",
    "\n",
    "paper = DocumentSchema.from_url(\n",
    "    url=\"https://aclanthology.org/L16-1417.pdf\",\n",
    "    title=\"Building Concept Graphs from Monolingual Dictionary Entries\",\n",
    "    doc_type=\"academic_paper\",\n",
    "    authors=[\"Gabor Recski\"],\n",
    "    conference=\"LREC\",\n",
    "    year=2016,\n",
    "    category=\"nlp\",\n",
    "    dataset_id=\"anthology\",\n",
    ")\n",
    "\n",
    "paper2 = DocumentSchema.from_url(\n",
    "    url=\"https://aclanthology.org/2025.bionlp-share.8.pdf\",\n",
    "    title=\"KR Labs at ArchEHR-QA 2025: A Verbatim Approach for Evidence-Based Question Answering\",\n",
    "    doc_type=\"academic_paper\",\n",
    "    authors=[\"Adam Kovacs\", \"Paul Schmitt\", \"Gabor Recski\"],\n",
    "    conference=\"BioNLP\",\n",
    "    year=2025,\n",
    "    category=\"nlp\",\n",
    "    dataset_id=\"anthology\",\n",
    ")\n",
    "\n",
    "paper3 = DocumentSchema.from_url(\n",
    "    url=\"https://aclanthology.org/2020.lrec-1.448.pdf\",\n",
    "    title=\"Better Together: Modern Methods Plus Traditional Thinking in NP Alignment\",\n",
    "    doc_type=\"academic_paper\",\n",
    "    authors=[\"Adam Kovacs\", \"Judit Acs\", \"Andras Kornai\", \"Gabor Recski\"],\n",
    "    conference=\"LREC\",\n",
    "    year=2020,\n",
    "    category=\"nlp\",\n",
    "    dataset_id=\"anthology\",\n",
    ")\n",
    "\n",
    "paper4 = DocumentSchema.from_url(\n",
    "    url=\"https://aclanthology.org/2020.msr-1.2.pdf\",\n",
    "    title=\"BME-TUW at SR‚Äô20: Lexical grammar induction for surface realization\",\n",
    "    doc_type=\"academic_paper\",\n",
    "    authors=[\n",
    "        \"G√°bor Recski\",\n",
    "        \"√Åd√°m Kov√°cs\",\n",
    "        \"Kinga G√©mes\",\n",
    "        \"Judit √Åcs\",\n",
    "        \"Andras Kornai\",\n",
    "    ],\n",
    "    conference=\"MSR\",\n",
    "    year=2020,\n",
    "    category=\"nlp\",\n",
    "    dataset_id=\"anthology\",\n",
    ")\n",
    "\n",
    "paper5 = DocumentSchema.from_url(\n",
    "    url=\"https://aclanthology.org/D19-6304.pdf\",\n",
    "    title=\"BME-UW at SRST-2019: Surface realization with Interpreted Regular Tree Grammars\",\n",
    "    doc_type=\"academic_paper\",\n",
    "    authors=[\"√Åd√°m Kov√°cs\", \"Evelin √Åcs\", \"Judit √Åcs\", \"Andras Kornai\", \"G√°bor Recski\"],\n",
    "    conference=\"SRST\",\n",
    "    year=2019,\n",
    "    category=\"nlp\",\n",
    "    dataset_id=\"anthology\",\n",
    ")\n",
    "\n",
    "paper6 = DocumentSchema.from_url(\n",
    "    url=\"https://aclanthology.org/2020.semeval-1.15.pdf\",\n",
    "    title=\"BMEAUT at SemEval-2020 Task 2: Lexical entailment with semantic graphs\",\n",
    "    doc_type=\"academic_paper\",\n",
    "    authors=[\"√Åd√°m Kov√°cs\", \"Kinga G√©mes\", \"Andras Kornai\", \"G√°bor Recski\"],\n",
    "    conference=\"SemEval\",\n",
    "    year=2020,\n",
    "    category=\"nlp\",\n",
    "    dataset_id=\"anthology\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda082f-d8a5-42c7-900f-39835324ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add_documents([paper, paper2, paper3, paper4, paper5, paper6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3fedf9-257b-4a93-943e-f0b1264804bf",
   "metadata": {},
   "source": [
    "\n",
    "### What Happens When You Add Documents?\n",
    "\n",
    "```python\n",
    "# Add documents\n",
    "index.add_documents([doc])\n",
    "```\n",
    "\n",
    "Behind the scenes, `VerbatimIndex` orchestrates a complex pipeline:\n",
    "\n",
    "1. **Chunking:** Calls `chunker.chunk(doc.content)` ‚Üí Returns `(raw, enhanced)` tuples\n",
    "2. **Metadata enrichment:** Appends document metadata to enhanced chunks:\n",
    "   ```\n",
    "   # Introduction\n",
    "   Machine learning...\n",
    "\n",
    "   ---\n",
    "   Document: KR Labs at ArchEHR-QA 2025\n",
    "   Source: https://aclanthology.org/2025.bionlp-share.8.pdf\n",
    "   Authors: ['Adam Kovacs', 'Paul Schmitt', 'Gabor Recski']\n",
    "   Year: 2025\n",
    "   Conference: BioNLP\n",
    "   ```\n",
    "3. **Batch embedding:** Calls `embedder.embed_batch(enhanced_chunks)`\n",
    "4. **Storage:** Stores vectors + metadata in Milvus\n",
    "\n",
    "üí° **Key Insight:** Raw text is stored for display/extraction, enhanced text is embedded for retrieval!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a6a138-5358-472c-839b-ff42b43de235",
   "metadata": {},
   "source": [
    "\n",
    "## Part 6: Inspecting Your Index\n",
    "\n",
    "Once you've indexed documents, you should inspect what got created. This is crucial for debugging and understanding your system.\n",
    "\n",
    "### Get Overview Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6afc091f-9122-4715-896f-c85f77faa7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 6\n",
      "Total chunks: 88\n",
      "Average chunks per doc: 14.7\n",
      "\n",
      "Document types:\n",
      "  academic_paper: 6\n",
      "\n",
      "Sample documents:\n",
      "  - BMEAUT at SemEval-2020 Task 2: Lexical entailment with semantic graphs (N/A)\n",
      "  - BME-UW at SRST-2019: Surface realization with Interpreted Regular Tree Grammars (N/A)\n",
      "  - Building Concept Graphs from Monolingual Dictionary Entries (N/A)\n"
     ]
    }
   ],
   "source": [
    "stats = index.inspect()\n",
    "print(f\"Total documents: {stats['total_documents']}\")\n",
    "print(f\"Total chunks: {stats['total_chunks']}\")\n",
    "print(f\"Average chunks per doc: {stats['total_chunks'] / stats['total_documents']:.1f}\")\n",
    "\n",
    "print(\"\\nDocument types:\")\n",
    "for doc_type, count in stats[\"doc_types\"].items():\n",
    "    print(f\"  {doc_type}: {count}\")\n",
    "\n",
    "print(\"\\nSample documents:\")\n",
    "for doc in stats[\"sample_documents\"][:3]:\n",
    "    print(f\"  - {doc['title']} ({doc.get('year', 'N/A')})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a4bef-4d00-4594-9a9f-034007d22354",
   "metadata": {},
   "source": [
    "\n",
    "### Examine Individual Chunks\n",
    "\n",
    "Let's look at what actually got stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de461237-ed6c-41e4-a5ba-d3eb15c650b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = index.get_all_chunks(limit=5)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Chunk {i}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    print(f\"\\nID: {chunk.id}\")\n",
    "    print(f\"Score: {chunk.score}\")\n",
    "\n",
    "    print(\"\\n--- Raw Text (for extraction/display) ---\")\n",
    "    print(chunk.text[:200] + \"...\")\n",
    "\n",
    "    print(\"\\n--- Enhanced Text (what was embedded) ---\")\n",
    "    print(chunk.enhanced_text[:300] + \"...\")\n",
    "\n",
    "    print(\"\\n--- Metadata ---\")\n",
    "    metadata_keys = [\"title\", \"authors\", \"year\", \"conference\", \"chunk_number\"]\n",
    "    for key in metadata_keys:\n",
    "        if key in chunk.metadata:\n",
    "            print(f\"  {key}: {chunk.metadata[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e74031f-0ac4-44a2-80ab-6adbf775d5e5",
   "metadata": {},
   "source": [
    "\n",
    "**This will show you:**\n",
    "- How your chunks look after processing\n",
    "- What metadata was preserved\n",
    "- The difference between raw and enhanced text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73896b3-3b00-40c1-8e76-b988e4e17df1",
   "metadata": {},
   "source": [
    "### Filter by Metadata\n",
    "\n",
    "Remember those custom fields you added? Now you can filter by them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2e1738a-66a9-4b48-a35c-7b32f8cc7bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19 chunks from 2025\n"
     ]
    }
   ],
   "source": [
    "# Get only 2025 papers\n",
    "chunks_2025 = index.query(\n",
    "    text=None,  # No semantic search, just filter\n",
    "    filter='metadata[\"year\"] == 2025',\n",
    "    k=100,\n",
    ")\n",
    "print(f\"Found {len(chunks_2025)} chunks from 2025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "295b7270-f2e2-4200-8233-57659053e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_bionlp = index.query(filter='metadata[\"conference\"] == \"BioNLP\"', k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8e5cd54-aefc-4c13-b221-84a17c0a5b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get chunks from a specific document\n",
    "doc_chunks = index.get_chunks_by_document(\"13354f18-c365-484c-bf6d-16a26401b73d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b98e3c-2ee6-489a-8932-b63b025a1d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
